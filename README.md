# gg-observability (P3) ‚Äî kube-prometheus-stack on EKS (Elastic Kubernetes Service)

## üéØ Goal
Deploy **kube-prometheus-stack** to an existing **EKS (Elastic Kubernetes Service)** cluster and document proof of:
- **Prometheus (metrics collector)** scraping targets
- **Grafana (dashboards)** access using **port-forward (local tunnel)**
- **Alerting** using a custom **PrometheusRule (Prometheus alert rules)**

## üß≠ Architecture
![Observability architecture](docs/diagrams/gg-observability-arch.png)

**Editable diagram (draw.io (diagrams.net))**
- `docs/diagrams/gg-observability-arch.drawio`

**PNG generated by code**
- `docs/diagrams/arch.py`

## üì¶ What‚Äôs in this repo (source of truth)
- `helm/values-kps.yaml`  
  - Helm (Helm package manager) values for **kube-prometheus-stack**  
  - Release label used in repo: **kps**
- `k8s/alerts-demo.yaml`  
  - **PrometheusRule (Prometheus alert rules)** demo rules (namespace: `monitoring`)
- `docs/screenshots/`  
  - Proof screenshots (nodes, pods, Grafana, Prometheus targets, alert firing)
- `docs/runbook.md`  
  - Steps to reproduce (when you rebuild a cluster later)
- `docs/evidence.md`  
  - Claim ‚Üí exact screenshot proof mapping

## ‚öôÔ∏è Key config (from repo files)
From `helm/values-kps.yaml`:
- **Grafana (dashboards) Service type**: **ClusterIP (internal service)**  
- **Grafana (dashboards) admin credentials**: read from existing Secret `kps-grafana-admin`
  - keys: `admin-user`, `admin-password`
- **Prometheus (metrics collector) retention**: **12h**
- **Resource requests** (examples present in file):
  - **Alertmanager (alert router)**: `cpu: 50m`, `memory: 128Mi`
  - **Prometheus (metrics collector)**: `cpu: 150m`, `memory: 512Mi`

## üßæ Proof
See: **`docs/evidence.md`**

## üßπ Cleanup (when you rebuild later)
- Uninstall Helm (Helm package manager) release **kps**
- Delete namespace `monitoring` (if you created it for this project)
- Stop any port-forward (local tunnel) sessions.
